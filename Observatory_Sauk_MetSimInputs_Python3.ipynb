{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# A Notebook to explore gridded climate time series data and DHSVM modeling of the Sauk-Suiattle Watershed  \n",
    "<img src= \"http://www.sauk-suiattle.com/images/Elliott.jpg\"\n",
    "style=\"float:left;width:200px;padding:20px\">   \n",
    "This data is compiled to digitally observe the Sauk-Suiattle Watershed, powered by HydroShare. <br />\n",
    "<br />\n",
    "*Use this Jupyter Notebook to:* <br /> \n",
    "Download and generate lists of gridded climate points for a watershed<br />\n",
    "Download Livneh daily 1/16 degree gridded climate data, <br /> \n",
    "Download WRF daily 1/16 degree gridded climate data, <br /> \n",
    "Visualize daily, monthly, and annual temperature and precipitation data. <br /> \n",
    "Calculate Long-term Mean Monthly Bias Corrections for WRF using Livneh Low Elevation data<br /> \n",
    "Bias correct each Livneh grid cell using bias corrected WRF (use to correct Livneh 2013 and MACA data). <br />\n",
    "Visualize daily, monthly, and annual temperature and precipitation data with corrected results. <br /> \n",
    "Update VIC model soil input (optional). <br /> \n",
    "Save results back to HydroShare. <br /> \n",
    " <br /> <br /> <img src=\"https://www.washington.edu/brand/files/2014/09/W-Logo_Purple_Hex.png\" style=\"float:right;width:120px;padding:20px\">  \n",
    "#### A Watershed Dynamics Model by the Watershed Dynamics Research Group in the Civil and Environmental Engineering Department at the University of Washington "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Livneh bias correction to WRF, that is bias corrected to the Livneh low elevation long term mean, which has been spatially averaged (with a few experimental GIS selections).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  HydroShare Setup and Preparation\n",
    "\n",
    "To run this notebook, we must import several libaries. These are listed in order of 1) Python standard libraries, 2) hs_utils library provides functions for interacting with HydroShare, including resource querying, dowloading and creation, and 3) the observatory_gridded_hydromet library that is downloaded with this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Python libraries available on CUAHSI JupyterHub \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "#HydroShare Utilities\n",
    "from utilities import hydroshare\n",
    "#UW Watershed Dynamics Lab Utilities\n",
    "#from utilities import observatory_gridded_hydromet as ogh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish a secure connection with HydroShare by instantiating the hydroshare class that is defined within hs_utils. In addition to connecting with HydroShare, this command also sets and prints environment variables for several parameters that will be useful for saving work back to HydroShare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the following system variables:\n",
      "   HS_USR_NAME = ChristinaBandaragoda\n",
      "   HS_RES_ID = f0f90f5645864e0d9c0e0209d0095d74\n",
      "   HS_RES_TYPE = genericresource\n",
      "   JUPYTER_HUB_IP = jupyter.cuahsi.org\n",
      "\n",
      "These can be accessed using the following command: \n",
      "   os.environ[key]\n",
      "\n",
      "   (e.g.)\n",
      "   os.environ[\"HS_USR_NAME\"]  => ChristinaBandaragoda\n",
      "Successfully established a connection with HydroShare\n"
     ]
    }
   ],
   "source": [
    "hs=hydroshare.hydroshare()\n",
    "#homedir = ogh.mapContentFolder(str(os.environ[\"HS_RES_ID\"]))\n",
    "#print('Data will be loaded from and save to:'+homedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious about where the data is being downloaded, click on the Jupyter Notebook dashboard icon to return to the File System view.  The homedir directory location printed above is where you can find the data and contents you will download to a HydroShare JupyterHub server.  At the end of this work session, you can migrate this data to the HydroShare iRods server as a Generic Resource. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mappingfile='/home/jovyan/work/notebooks/data/f0f90f5645864e0d9c0e0209d0095d74/f0f90f5645864e0d9c0e0209d0095d74/data/contents/monkeysonatree.csv'\n",
    "statefileexample='/home/jovyan/work/notebooks/data/MetSim-develop/tests/data/state.nc'\n",
    "\n",
    "from utilities import tonic\n",
    "import xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating grid size now...\n",
      "found 14 unique lons\n",
      "found 11 unique lats\n",
      "Created a target grid based on the lats and lons in the input file names\n",
      "Grid Size: (11, 14)\n",
      "{'lon': (array([-121.7188, -121.6562, -121.5938, -121.5312, -121.4688, -121.4062,\n",
      "       -121.3438, -121.2812, -121.2188, -121.1562, -121.0938, -121.0312,\n",
      "       -120.9688, -120.9062]), ('lon',), {'units': 'degrees_east', 'long_name': 'longitude coordinate'}), 'mask': array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "       [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'lat': (array([ 47.9062,  47.9688,  48.0312,  48.0938,  48.1562,  48.2188,\n",
      "        48.2812,  48.3438,  48.4062,  48.4688,  48.5312]), ('lat',), {'units': 'degrees_north', 'long_name': 'latitude coordinate'})}\n"
     ]
    }
   ],
   "source": [
    "map_df= pd.read_csv(mappingfile)\n",
    "#print(map_df)\n",
    "\n",
    "data = tonic.calc_grid(map_df.LAT, map_df.LONG_, decimals=4)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 1 1 1 0 0 0 0 0]\n",
      " [0 0 0 1 1 1 1 1 1 1 1 0 0 0]\n",
      " [0 0 0 0 1 1 1 1 1 1 1 1 0 0]\n",
      " [0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [0 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
      " [0 0 1 1 1 1 1 1 1 1 1 1 1 0]\n",
      " [0 1 1 1 1 1 1 1 1 1 1 0 0 0]\n",
      " [0 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataA=data['mask']\n",
    "print(dataA)\n",
    "dataA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 11, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3 = np.expand_dims(dataA, axis=0)\n",
    "data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = xarray.Dataset({'mask': (['x', 'y', 'time'],  data3)}                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds.to_netcdf('sauk_domain.nc')\n",
    "!cp 'sauk_domain.nc' /home/jovyan/work/notebooks/data/MetSim-develop/tests/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=98, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(map_df.LONG_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 15 + 8 * np.random.randn(11, 14, 3)\n",
    "\n",
    "precip = 10 * np.random.rand(11, 14, 3)\n",
    "\n",
    "lon = data['lon']#[[-99.83, -99.32], [-99.79, -99.23]]\n",
    "\n",
    "lat = data['lat'] #[[42.25, 42.21], [42.63, 42.59]]\n",
    "\n",
    "# for real use cases, its good practice to supply array attributes such as\n",
    "# units, but we won't bother here for the sake of brevity\n",
    "stateds = xarray.Dataset({'mask': (['x', 'y', 'time'],  data3),\n",
    "                         'time': pd.date_range('1950-01-01', periods=14),\n",
    "                         'reference_time': pd.Timestamp('1951-12-31')})\n",
    "stateds.to_netcdf('sauk_state.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12186000.streamflow.daily.cms.txt\r\n",
      "12189500.streamflow.daily.cms.txt\r\n",
      "avg_monthly_temp.png\r\n",
      "ChocolateGlacier.streamflow.daily.cms.txt\r\n",
      "CoolGlacier.streamflow.daily.cms.txt\r\n",
      "data_48.15625_-121.28125\r\n",
      "domain.nc\r\n",
      "DustyGlacier.streamflow.daily.cms.txt\r\n",
      "ELA_all_6_7.txt\r\n",
      "ErmineGlacier.streamflow.daily.cms.txt\r\n",
      "gl_cov_glac_6_7.txt\r\n",
      "HoneycombGlacier.streamflow.daily.cms.txt\r\n",
      "monkeysonatree.csv\r\n",
      "month_Livneh1915_2011.csv\r\n",
      "month_WRF_CIG_1950_2010.csv\r\n",
      "observatory_gridded_hydromet_072117.py\r\n",
      "observatory_gridded_hydromet_Jimmys082217version.py\r\n",
      "observatory_gridded_hydromet.py\r\n",
      "Observatory_Sauk_071817.ipynb\r\n",
      "Observatory_Sauk_072117.ipynb\r\n",
      "Observatory_Sauk_LivBC2WRF_090817.ipynb\r\n",
      "Observatory_Sauk_LivBC2WRF_091417.ipynb\r\n",
      "Observatory_Sauk_LivBC2WRF_091817.ipynb\r\n",
      "Observatory_Sauk_Livbc2WRFbc2LivLlowelevltm.ipynb\r\n",
      "Observatory_Sauk_LivLovWRFltm_MetBiasCorrection.ipynb\r\n",
      "Observatory_Sauk_LivnehBCtoWRF.ipynb\r\n",
      "Observatory_Sauk_MetBiasCorrection.ipynb\r\n",
      "Observatory_Sauk_MetHighElevBiasCorrection.ipynb\r\n",
      "Observatory_Sauk_MetMidHighElevBiasCorrection.ipynb\r\n",
      "Observatory_Sauk_MetSimInputs_Python3.ipynb\r\n",
      "Observatory_Sauk_TreatGeoSelf.ipynb\r\n",
      "Observatory_Sauk_WatershedHydroMetDownload.ipynb\r\n",
      "Observatory_Skagit_LivLovWRFltm_MetBiasCorrection.ipynb\r\n",
      "Precip.daily.Liv2013m.txt\r\n",
      "Precip.daily.WRF.m.txt\r\n",
      "Sauk_30LowElevCentroidVICpoints.txt\r\n",
      "sauk_domain.nc\r\n",
      "SaukRiver_Sauk_obs_cfs.dly\r\n",
      "sauk_state.nc\r\n",
      "SpatialFindData.ipynb\r\n",
      "state.nc\r\n",
      "SuiattleGlacier.streamflow.daily.cms.txt\r\n",
      "ThunderBasinSNOTEL.txt\r\n",
      "tonic.py\r\n",
      "USC00455678.csv\r\n",
      "VistaGlacier.streamflow.daily.cms.txt\r\n"
     ]
    }
   ],
   "source": [
    "# use bash to look at your files\n",
    "#!ls\n",
    "!cp sauk_state.nc ../../../../MetSim-develop/tests/data\n",
    "!cp sauk_domain.nc ../../../../MetSim-develop/tests/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run MetSim to dissagregate daily data to 3-hrly DHSVM inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get scripts from clone or download on JupyterHub server \n",
    "!cp /home/jovyan/work/notebooks/data/MetSim-develop/* . \n",
    "\n",
    "\n",
    "\n",
    "import MetSim-develop.metsim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the results back into HydroShare\n",
    "<a name=\"creation\"></a>\n",
    "\n",
    "Using the `hs_utils` library, the results of the Geoprocessing steps above can be saved back into HydroShare.  First, define all of the required metadata for resource creation, i.e. *title*, *abstract*, *keywords*, *content files*.  In addition, we must define the type of resource that will be created, in this case *genericresource*.  \n",
    "\n",
    "***Note:*** Make sure you save the notebook at this point, so that all notebook changes will be saved into the new HydroShare resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move each file on the server within the 'files' list to an :EXISTING\" HydroShare Generic Resource content folder.  Parent_resource is the destination resource ID for an existing Generic Resource. Files is a list of filepaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Talk to CUAHSI about why the add file function is throwing errors\n",
    "\"\"\"\n",
    "#parent_resource = '0236ae196d204f1cba421787f38dec71'\n",
    "#files= ['Observatory_Sauk_071117.ipynb'] #os.path.join(homedir,)\n",
    "\n",
    "#response_json = hs.addContentToExistingResource(resid=parent_resource, content=files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ThisNotebook='Observatory_Sauk_LivBC2WRF_091417.ipynb' #check name for consistency\n",
    "liv2013_tar = 'livneh2013.tar.gz'\n",
    "wrf_tar = 'Salathe2014.tar.gz'\n",
    "biascorrWRF_liv_tar = 'biascorrWRF_WRFbc.tar.gz'\n",
    "\n",
    "!tar -zcf {liv2013_tar} livneh2013\n",
    "!tar -zcf {wrf_tar} Salathe2014\n",
    "!tar -zcf {biascorrWRF_liv_tar} biascorrWRF_liv\n",
    "\n",
    "files=[ThisNotebook,\n",
    "       'observatory_gridded_hydromet.py',\n",
    "       liv2013_tar,\n",
    "       wrf_tar,\n",
    "       biascorrWRF_liv_tar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for each file downloaded onto the server folder, move to a new HydroShare Generic Resource\n",
    "title = 'Sauk-Suiattle Observatory Bias Correction Results - Livneh 2013 to WRFbc'\n",
    "abstract = 'This output is a bias correction test to generate a hybrid gridded meteorology product'\n",
    "keywords = ['Sauk', 'climate', 'WRF','hydrometeorology'] \n",
    "rtype = 'genericresource'  \n",
    "\n",
    "# create the new resource\n",
    "resource_id = hs.createHydroShareResource(abstract, \n",
    "                                          title,\n",
    "                                          keywords=keywords, \n",
    "                                          resource_type=rtype, \n",
    "                                          content_files=files, \n",
    "                                          public=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
