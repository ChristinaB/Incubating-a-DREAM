{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# A Notebook to prepare gridded climate time series data for DHSVM modeling of the Skagit Watershed  <br />\n",
    "This workflow was originally developed to digitally observe the Sauk-Suiattle Watershed, and is expanded here to the Skagit watershed. Low elevation observations are used to correct modeled atmospheric data based on the differences in the long term mean monthly temperature and precipitation. This is expected to improve hydrologic modeling at low elevations, and we are prioritizing predictions of low flows in low elevation tributaries. Limited analysis conducted to date shows that this approach may not be an accurate representation of high elevation processes.  This is an area of active research.\n",
    "<br /><br />\n",
    "<img src= \"http://www.sauk-suiattle.com/images/Elliott.jpg\"\n",
    "style=\"float:left;width:200px;padding:20px\">   \n",
    "\n",
    "*Use this Jupyter Notebook to:* <br /> \n",
    "Download and generate lists of gridded climate points<br />\n",
    "Download Livneh daily 1/16 degree gridded climate data, <br /> \n",
    "Download WRF daily 1/16 degree gridded climate data, <br /> \n",
    "Visualize daily, monthly, and annual temperature and precipitation data. <br /> \n",
    "Calculate Long-term Mean Monthly Bias Corrections for WRF using Livneh Low Elevation data<br /> \n",
    "Bias correct each Livneh grid cell using bias corrected WRF (use to correct Livneh 2013 and MACA data). <br />\n",
    "Visualize daily, monthly, and annual temperature and precipitation data with corrected results. <br /> \n",
    "Update VIC model soil input (optional). <br /> \n",
    "Save results back to HydroShare. <br /> \n",
    " <br /> <br /> <img src=\"https://www.washington.edu/brand/files/2014/09/W-Logo_Purple_Hex.png\" style=\"float:right;width:120px;padding:20px\">  \n",
    "#### A Watershed Dynamics Model by the Watershed Dynamics Research Group in the Civil and Environmental Engineering Department at the University of Washington "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  HydroShare Setup and Preparation\n",
    "\n",
    "To run this notebook, we must import several libaries. These are listed in order of 1) Python standard libraries, 2) hs_utils library provides functions for interacting with HydroShare, including resource querying, dowloading and creation, and 3) the observatory_gridded_hydromet library that is downloaded with this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Python libraries available on CUAHSI JupyterHub \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#HydroShare Utilities\n",
    "from utilities import hydroshare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time=datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UPDATE ME 1b.  Import python scripts from a Github repository\n",
    "#### Please see the [Observatory]( https://github.com/Freshwater-Initiative/Observatory/blob/master/README.md) repository on Github with a Readme instructions on how to use Git and the JupyterHub server terminal to push/pull changes.   After completing the steps to get the observatory_gridded_hydromet.py script into your HydroShare Utilities folder, execute the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ogh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish a secure connection with HydroShare by instantiating the hydroshare class that is defined within hs_utils. In addition to connecting with HydroShare, this command also sets and prints environment variables for several parameters that will be useful for saving work back to HydroShare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hs=hydroshare.hydroshare()\n",
    "homedir = ogh.mapContentFolder(str(os.environ[\"HS_RES_ID\"]))\n",
    "print('Data will be loaded from and save to:'+homedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious about where the data is being downloaded, click on the Jupyter Notebook dashboard icon to return to the File System view.  The homedir directory location printed above is where you can find the data and contents you will download to a HydroShare JupyterHub server.  At the end of this work session, you can migrate this data to the HydroShare iRods server as a Generic Resource. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and generate lists of gridded climate points for a watershed\n",
    "Retrieve a list of grid points and configuration file from a HydroShare resource\n",
    "This example uses a ascii text that is stored in HydroShare at the following url: https://www.hydroshare.org/resource/d90289409f904017831d308642c1eb30/ . The data for our processing routines can be retrieved using the getResourceFromHydroShare function by passing in the global identifier from the url above.  In the next cell, we download this resource from HydroShare, and identify that the table in this resource is the 'mappingfile' variable identifying the Lat/Long points to be used for downloading hydrometeorology data.  The file must include columns with station numbers (this can be aribitrary), latitude, longitude, and elevation. The header of these columns must be FID, LAT, LONG_, and ELEV or RASTERVALU, respectively. The station numbers will be used for the remainder of the code to uniquely reference data from each climate station, as well as to identify minimum, maximum, and average elevation of all of the climate stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User provides their HydroShare resource ID from their own polgyon shapefile uploaded to HydroShare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs.getResourceFromHydroShare('b4c129d29e5e4452985c9d8dc0fe01ed')\n",
    "shapefile = hs.content['SkagitRiver_BasinBoundary.shp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the exising point shapefile of available 1/16 degree grid centroid locations shared on HydroShare as a public resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs.getResourceFromHydroShare('ef2d82bf960144b4bfb1bae6242bcc7f')\n",
    "NAmer = hs.content['NAmer_dem_list.shp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use TreatGeoSelf to generate a list of lat/long points in your area of interest\n",
    "The TreatGeoSelf() function was designed to easily generate a list of lat/long points in your area of interest\n",
    "Use the buffer distance in decimal degrees to select points within your polygon (distance=0) or within a 1/16 degree buffer outside of the polygon (distance = 0.0625)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappingfile = ogh.treatgeoself(shapefile=shapefile, NAmer=NAmer, folder_path=os.getcwd(), outfilename='monkeysonatree.csv', buffer_distance=0.00)\n",
    "print(mappingfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Location Name and watershed drainage area (m2)\n",
    "#### Watershed information upstream of USGS 12200500 Skagit River near Mount Vernon, WA  \n",
    "Location.--Latitude 48°26'42\", Longitude 122°20'03\", in SE 1/4 SE 1/4 Section 7, Township 34 North, Range 4 East, in Skagit County, Hydrologic Unit 17110007, on right bank 220 feet downstream of bridge on U.S. Highway 99, 1.5 miles north of Skagit Valley Junior College in Mount Vernon, and at river mile 15.7. Drainage area is 3,093 mi2, of which 400 mi2 is in Canada. Datum of gage is NGVD of 1929. \n",
    "https://waterdata.usgs.gov/nwis/uv?site_no=12200500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loc_name='Skagit Watershed'\n",
    "streamflow_watershed_drainage_area=8010833000 # square meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the Observatory metadata file \n",
    "This file contains the variables, data types and metadata related to Livneh et al., 2013; 2015 and Salathe et al., 2014 gridded hydrometeorology products. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assuming this is pulled from Github, how can we import this from Utilities.\n",
    "#Otherwise it needs to be in each HydroShare resource - which if fine too. \n",
    "with open('ogh_meta.json','r') as r:\n",
    "    meta_file = json.load(r)\n",
    "    r.close()\n",
    "\n",
    "sorted(meta_file.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Download climate data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Daily Meteorologic Data (1915-2011) from Livneh et al. 2013 \n",
    "\n",
    "The functions used in this section apply to hydrometeorology data within the Continental United States with daily data 1915-2011. <br/>\n",
    "View data extent at  Livneh, B. (2017). Gridded climatology locations (1/16th degree): Continental United States extent, HydroShare, http://www.hydroshare.org/resource/14f0a6619c6b45cc90d1f8cabc4129af\n",
    "\n",
    "Please cite: <br/>\n",
    "Livneh B., E.A. Rosenberg, C. Lin, B. Nijssen, V. Mishra, K.M. Andreadis, E.P. Maurer, and D.P. Lettenmaier, 2013: A Long-Term Hydrologically Based Dataset of Land Surface Fluxes and States for the Conterminous United States: Update and Extensions, Journal of Climate, 26, 9384–9392.<br/>\n",
    "<br/>\n",
    "The getClimateData_DailyMET_livneh2013() function reads in the mapping file table, downloads, and unzips the data files for each of the longitude and latitude points. The folder containing the data is within the directory listed as homedir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ogh.getDailyMET_livneh2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Daily_MET_1915_2011 = ogh.getDailyMET_livneh2013(homedir, mappingfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Get Daily Weather Research and Forecasting (WRF 1950-2010 Pacific Northwest) from Salathe et al., 2014\n",
    "<br/>\n",
    "Please cite 2014 data using: <br/>\n",
    "Salathé, EP, AF Hamlet, CF Mass, M Stumbaugh, S-Y Lee, R Steed: 2017. Estimates of 21st Century Flood Risk in the Pacific Northwest Based on Regional Scale Climate Model Simulations.  J. Hydrometeorology. DOI: 10.1175/JHM-D-13-0137.1\n",
    "\n",
    "This data is also available on HydroShare and can be downloaded using the following line of code (copy into code block):\n",
    "hs.getResourceFromHydroShare('0db969e4cfb54cb18b4e1a2014a26c82')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ogh.getDailyWRF_salathe2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Daily_WRFraw_1950_2010 = ogh.getDailyWRF_salathe2014(homedir, mappingfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful hint: Jupyter Notebooks on the CUAHSI JupyterHub server on ROGER supercomputer can use Python or bash command line coding to explore the data folders.  Alternatively, you can click on the orange Jupyter icon in the upper left corner to open the folder view to see where in the world your files are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('This is the list of folders in your directory for this HydroShare resource.')\n",
    "test = [each for each in os.listdir(homedir) if os.path.isdir(each)]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Calculate Long-term Monthly Bias Corrections for WRF using Livneh Low Elevation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take two tuples representing start and end date ranges, then find their overlapping date_range\n",
    "dr1 = meta_file['dailymet_livneh2013']['date_range']\n",
    "dr2 = meta_file['dailywrf_salathe2014']['date_range']\n",
    "\n",
    "dr = ogh.overlappingDates(tuple([dr1['start'],dr1['end']]), tuple([dr2['start'],dr2['end']]))\n",
    "dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#initiate new dictionary with original data\n",
    "ltm_0to3000 = ogh.gridclim_dict(gridclim_folder='livneh2013_MET',\n",
    "                               loc_name=loc_name,\n",
    "                               dataset='dailymet_livneh2013',\n",
    "                               mappingfile=mappingfile, \n",
    "                               metadata=meta_file,\n",
    "                               file_start_date=None, \n",
    "                               file_end_date=None,\n",
    "                               subset_start_date=dr[0],\n",
    "                               subset_end_date=dr[1])\n",
    "\n",
    "ltm_0to3000 = ogh.gridclim_dict(gridclim_folder='livneh2013_MET',\n",
    "                               loc_name=loc_name,\n",
    "                               dataset='dailywrf_salathe2014',\n",
    "                               mappingfile=mappingfile, \n",
    "                               metadata=meta_file,\n",
    "                               file_start_date=None, \n",
    "                               file_end_date=None,\n",
    "                               subset_start_date=dr[0],\n",
    "                               subset_end_date=dr[1],\n",
    "                               df_dict=ltm_0to3000)\n",
    "\n",
    "sorted(ltm_0to3000.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Perform bias correction using differences between WRFbc (from 4) and Liv2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BiasCorr_wrfbc = ogh.compute_diffs(df_dict=ltm_0to3000, df_str='0to3000m',\n",
    "                                   gridclimname1='dailywrf_salathe2014',\n",
    "                                   gridclimname2='dailymet_livneh2013',\n",
    "                                   prefix2=['month'],\n",
    "                                   prefix1=meta_file['dailymet_livneh2013']['variable_list'])\n",
    "\n",
    "BiasCorr_wrfbc_P = ogh.compute_ratios(df_dict=ltm_0to3000, df_str='0to3000m',\n",
    "                                      gridclimname1='dailywrf_salathe2014',\n",
    "                                      gridclimname2='dailymet_livneh2013',\n",
    "                                      prefix2=['month'],\n",
    "                                      prefix1=meta_file['dailymet_livneh2013']['variable_list'])\n",
    "\n",
    "           \n",
    "BiasCorr_wrfbc['PRECIP_0to3000m'] = BiasCorr_wrfbc_P['PRECIP_0to3000m']\n",
    "\n",
    "print('Precipitation values are a ratio of WRF_m/Liv_m and Temperature values are the difference between WRF_m-Liv_m')\n",
    "\n",
    "print(sorted(BiasCorr_wrfbc.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the BiasCorr_wrfbc dictionary of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ogh.saveDictOfDf(outfilename='BiasCorr_wrfbc.json', dictionaryObject=BiasCorr_wrfbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to read biascorr json objects back in, use readDictOfDF\n",
    "#testingobject = readDictOfDf(infilename='BiasCorr_wrfbc_test.json')\n",
    "#testingobject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Daily_MET_1915_2011_WRFbc_liv, meta_file = ogh.makebelieve(homedir=homedir,\n",
    "                                                           mappingfile=mappingfile,\n",
    "                                                           BiasCorr=BiasCorr_wrfbc,\n",
    "                                                           metadata=meta_file,\n",
    "                                                           start_catalog_label='dailymet_livneh2013',\n",
    "                                                           end_catalog_label='dailymet_livneh2013_wrfbc', \n",
    "                                                           file_start_date=None,\n",
    "                                                           file_end_date=None,\n",
    "                                                           data_dir=None,\n",
    "                                                           dest_dir_suffix='biascorrWRF_liv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#initiate new dictionary with original data\n",
    "ltm_0to3000 = ogh.gridclim_dict(gridclim_folder='livneh2013_MET',\n",
    "                                loc_name=loc_name,\n",
    "                                dataset='dailymet_livneh2013_wrfbc',\n",
    "                                mappingfile=mappingfile, \n",
    "                                metadata=meta_file,\n",
    "                                file_colnames=None,\n",
    "                                file_delimiter=None,\n",
    "                                file_start_date=None, \n",
    "                                file_end_date=None,\n",
    "                                file_time_step=None,\n",
    "                                subset_start_date=dr[0],\n",
    "                                subset_end_date=dr[1],\n",
    "                                df_dict=ltm_0to3000)\n",
    "\n",
    "#sorted(ltm_0to3000.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Low elevation bias correction\n",
    "\n",
    "Use monthly averages to correct the WRF values by the low elevation Livneh data.   Below, two new bias correction dataframes the same shape as BiasCorr are created to test the difference between correcting to Tmin and Tmax (global), and correcting both Tmin and Tmax by Tavg (global_Tavg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BiasCorr_wrfbc['PRECIP_0to3000m'].shape)\n",
    "#Make two new bias correction dataframes the same shape as BiasCorr\n",
    "BiasCorr_wrfbc_lowLiv=BiasCorr_wrfbc\n",
    "\n",
    "#Files used to generate low elevation correction factors\n",
    "#hs.getResourceFromHydroShare('ff886a1e191e47fd9ba13c23922741da')\n",
    "#shapefile_low = hs.content['SkagitLowElevationGrid_lessthan550_15k_poly.shp']\n",
    "#The monthly mean values are calculated in Observatory_Sauk_LivBC2WRFlow_092317.ipynb for 75 climate grid cells within a 15km buffer of the Skagit Watershed and are less than 550m\n",
    "global_lowelev_precip=[-2.840475,-2.513567,-2.397790,-2.276174,-1.639153,-1.589924,-0.686511,-0.541030,-0.750094,-1.777611,-2.442535,-2.629715]\n",
    "global_lowelev_tmax=[1.799996,2.804549,3.695474,3.849152,3.673436,3.562770,3.835815,3.839669,2.552046,1.718202,1.442429,1.314274]\n",
    "global_lowelev_tmin=[-1.298700,-1.045357,-0.143184,-0.127852,0.371726,0.868289,0.427259,0.085887,-1.208515,-2.180547,-1.978898,-1.578709]\n",
    "global_lowelev_tavg=[0.250648,0.879596,1.776145,1.860650,2.022581,2.215530,2.131537,1.962778,0.671766,-0.231173,-0.268234,-0.132217]\n",
    "global_wind=[0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "for column in BiasCorr_wrfbc_lowLiv['PRECIP_0to3000m']:\n",
    "    BiasCorr_wrfbc_lowLiv['PRECIP_0to3000m'].ix[1:12,column]=global_lowelev_precip\n",
    "    BiasCorr_wrfbc_lowLiv['TMAX_0to3000m'].ix[1:12,column]=global_lowelev_tmax\n",
    "    BiasCorr_wrfbc_lowLiv['TMIN_0to3000m'].ix[1:12,column]=global_lowelev_tmin\n",
    "    BiasCorr_wrfbc_lowLiv['WINDSPD_0to3000m'].ix[1:12,column]=global_wind\n",
    "print(BiasCorr_wrfbc_lowLiv['PRECIP_0to3000m'])\n",
    "print(BiasCorr_wrfbc_lowLiv['TMAX_0to3000m'])\n",
    "print(BiasCorr_wrfbc_lowLiv['TMIN_0to3000m'])\n",
    "print(BiasCorr_wrfbc_lowLiv['WINDSPD_0to3000m'])  \n",
    "\n",
    "Daily_MET_1915_2011_WRFbc_liv_global, meta_file = ogh.makebelieve(homedir=homedir,\n",
    "                                                                  mappingfile=mappingfile,\n",
    "                                                                  BiasCorr=BiasCorr_wrfbc_lowLiv,\n",
    "                                                                  metadata=meta_file,\n",
    "                                                                  start_catalog_label='dailymet_livneh2013_wrfbc',\n",
    "                                                                  end_catalog_label='dailymet_livneh2013_wrfbc_global', \n",
    "                                                                  file_start_date=None,\n",
    "                                                                  file_end_date=None,\n",
    "                                                                  data_dir=None,\n",
    "                                                                  dest_dir_suffix='biascorrWRF_global')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the correction factors for use with future climate data corrections using MACA Livneh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ogh.saveDictOfDf(outfilename='BiasCorr_wrfbc_lowLiv.json', dictionaryObject=BiasCorr_wrfbc_lowLiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the corrected time series to the dictionary with the raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm_0to3000 = ogh.gridclim_dict(gridclim_folder='livneh2013_MET',\n",
    "                                loc_name=loc_name,\n",
    "                                dataset='dailymet_livneh2013_wrfbc_global',\n",
    "                                mappingfile=mappingfile, \n",
    "                                metadata=meta_file,\n",
    "                                file_colnames=None,\n",
    "                                file_delimiter=None,\n",
    "                                file_start_date=None, \n",
    "                                file_end_date=None,\n",
    "                                file_time_step=None,\n",
    "                                subset_start_date=dr[0],\n",
    "                                subset_end_date=dr[1],\n",
    "                                df_dict=ltm_0to3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of variables and statistics calculated; available for visualization and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(ltm_0to3000.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv(mappingfile).sort_values('ELEV')\n",
    "temp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and compare monthly maximum temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare monthly averages for TMAX using livneh, salathe, and the salathe-corrected livneh\n",
    "comp = ['month_TMAX_dailymet_livneh2013','month_TMAX_dailymet_livneh2013_wrfbc','month_TMAX_dailywrf_salathe2014','month_TMAX_dailymet_livneh2013_wrfbc_global']\n",
    "\n",
    "obj = dict()\n",
    "for eachkey in ltm_0to3000.keys():\n",
    "    if eachkey in comp:\n",
    "        obj[eachkey] = ltm_0to3000[eachkey] \n",
    "panel_obj = pd.Panel.from_dict(obj)\n",
    "print(panel_obj)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "lws=[3, 10, 3, 3]\n",
    "styles=['b--','go-','y--','ro-']\n",
    "\n",
    "for col, style, lw in zip(comp, styles, lws):\n",
    "    #panel_obj.xs(key=(0.0, 48.53125, -121.59375), axis=2)[col].plot(style=style, lw=lw, ax=ax, legend=True)\n",
    "    #0    0  48.84375 -121.15625  1483.0\n",
    "\n",
    "    #0, 49.28125, -120.84375, 1727.0\n",
    "    panel_obj.xs(key=(0, 49.28125, -120.84375), axis=2)[col].plot(style=style, lw=lw, ax=ax, legend=True)    \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "lws=[3, 10, 3, 3]\n",
    "styles=['b--','go-','y--','ro-']\n",
    "\n",
    "for col, style, lw in zip(comp, styles, lws):\n",
    "    #    74\t48.59375\t-121.71875\t305\n",
    "    \n",
    "    # 176, 48.46875, -122.28125, 16.0\n",
    "    panel_obj.xs(key=(176, 48.46875, -122.28125), axis=2)[col].plot(style=style, lw=lw, ax=ax, legend=True)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and compare monthly precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare monthly averages for TMAX using livneh, salathe, and the salathe-corrected livneh\n",
    "comp = ['month_PRECIP_dailymet_livneh2013','month_PRECIP_dailymet_livneh2013_wrfbc','month_PRECIP_dailywrf_salathe2014','month_PRECIP_dailymet_livneh2013_wrfbc_global']\n",
    "\n",
    "obj = dict()\n",
    "for eachkey in ltm_0to3000.keys():\n",
    "    if eachkey in comp:\n",
    "        obj[eachkey] = ltm_0to3000[eachkey] \n",
    "panel_obj = pd.Panel.from_dict(obj)\n",
    "print(panel_obj)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "lws=[3, 10, 3, 3]\n",
    "styles=['b--','go-','y--','ro-']\n",
    "\n",
    "for col, style, lw in zip(comp, styles, lws):\n",
    "    #panel_obj.xs(key=(0.0, 48.53125, -121.59375), axis=2)[col].plot(style=style, lw=lw, ax=ax, legend=True)\n",
    "    \n",
    "    #0.0, 48.84375, -121.15625\n",
    "    panel_obj.xs(key=(0, 49.28125, -120.84375), axis=2)[col].plot(style=style, lw=lw, ax=ax, legend=True)    \n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "lws=[3, 10, 3, 3]\n",
    "styles=['b--','go-','y--','ro-']\n",
    "\n",
    "for col, style, lw in zip(comp, styles, lws):\n",
    "    \n",
    "    #74.0, 48.59375, -121.71875\n",
    "    panel_obj.xs(key=(176, 48.46875, -122.28125), axis=2)[col].plot(style=style, lw=lw, ax=ax, legend=True)    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (optional) - Run MetSim to dissagregate daily data to 3-hrly DHSVM inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Observatory_Sauk_MetSim_Python3.ipynb](https://jupyter.cuahsi.org/user/christinabandaragoda/notebooks/notebooks/data/f0f90f5645864e0d9c0e0209d0095d74/f0f90f5645864e0d9c0e0209d0095d74/data/contents/Observatory_Sauk_MetSim_Python3.ipynb) \n",
    "Edit this Markdown code hyperlink to your User Name if the Sauk Observatory HydroShare resource has been downloaded to your CUAHSI JupyterHub server personal user space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. (optional) - Update VIC model input file: soil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ogh.switchUpVICSoil(input_file=os.path.join(homedir,'soil_base'),\n",
    "                    output_file='soil',\n",
    "                    mappingfile=mappingfile,\n",
    "                    homedir=homedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the results back into HydroShare\n",
    "<a name=\"creation\"></a>\n",
    "\n",
    "Using the `hs_utils` library, the results of the Geoprocessing steps above can be saved back into HydroShare.  First, define all of the required metadata for resource creation, i.e. *title*, *abstract*, *keywords*, *content files*.  In addition, we must define the type of resource that will be created, in this case *genericresource*.  \n",
    "\n",
    "***Note:*** Make sure you save the notebook at this point, so that all notebook changes will be saved into the new HydroShare resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This is the list of folders in your directory for this HydroShare resource.')\n",
    "test = [each for each in os.listdir(homedir) if os.path.isdir(each)]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move each file on the server within the 'files' list to an :EXISTING\" HydroShare Generic Resource content folder.  Parent_resource is the destination resource ID for an existing Generic Resource. Files is a list of filepaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ThisNotebook='Observatory_Skagit_LivBC2WRFlow_122117.ipynb' #check name for consistency\n",
    "\n",
    "liv2013_tar = 'livneh2013.tar.gz'\n",
    "wrf_tar = 'salathe2014.tar.gz'\n",
    "biascorrWRF_liv_tar = 'biascorrWRF_liv.tar.gz'\n",
    "biascorrWRF_global_tar = 'biascorrWRF_global.tar.gz'\n",
    "\n",
    "!tar -zcf {liv2013_tar} livneh2013\n",
    "!tar -zcf {wrf_tar} salathe2014\n",
    "!tar -zcf {biascorrWRF_liv_tar} biascorrWRF_liv\n",
    "!tar -zcf {biascorrWRF_global_tar} biascorrWRF_global\n",
    "\n",
    "observatory_gridded_hydromet='ogh.py'\n",
    "soil = 'soil'\n",
    "CorrectionFactors_wrfliv='BiasCorr_wrfbc.json'\n",
    "CorrectionFactors_lowliv='BiasCorr_wrfbc_lowLiv.json'\n",
    "listofgridpoints ='monkeysonatree.csv'\n",
    "\n",
    "files=[ThisNotebook,\n",
    "       liv2013_tar,\n",
    "       observatory_gridded_hydromet,\n",
    "       wrf_tar,\n",
    "       biascorrWRF_liv_tar,biascorrWRF_global_tar,\n",
    "       soil,listofgridpoints,\n",
    "       CorrectionFactors_wrfliv,CorrectionFactors_lowliv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file downloaded onto the server folder, move to a new HydroShare Generic Resource\n",
    "title = 'Skagit Observatory Bias Correction Results - Livneh et al., 2013 to WRF (Salathe et al., 2014) and low elevation spatial average correction.'\n",
    "abstract = 'This output is a bias correction test to generate a hybrid gridded meteorology product. This dataset was generated December 21, 2017 using Observatory code from https://github.com/ChristinaB/Observatory.'\n",
    "keywords = ['Sauk', 'climate', 'WRF','hydrometeorology'] \n",
    "rtype = 'genericresource'  \n",
    "\n",
    "# create the new resource\n",
    "resource_id = hs.createHydroShareResource(abstract, \n",
    "                                          title,\n",
    "                                          keywords=keywords, \n",
    "                                          resource_type=rtype, \n",
    "                                          content_files=files, \n",
    "                                          public=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
